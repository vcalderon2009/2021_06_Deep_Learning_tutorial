{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dacf7e8-725b-4616-8b4f-700c56f9eb16",
   "metadata": {},
   "source": [
    "# Detección de Objectos\n",
    "\n",
    "En este tutorial veremos como se puede utilizar Pytorch para entrenar un modelo de detección\n",
    "de objetos.\n",
    "\n",
    "Usaremos la base de datos [*Penn-Fudan Database for Pedestrian Detection and Segmentation*](https://www.cis.upenn.edu/~jshi/ped_html/)\n",
    "y la entrenaremos utilizando un modelo de [Mask R-CNN](https://arxiv.org/abs/1703.06870). Este modelu incluye un total de\n",
    "170 imágenes con 345 instancias de peatones, y lo usaremos para ver como podemos extraer \"features\" para entrenar un modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a0834-e540-4f49-807e-7d4bfea0dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos los paquetes necesarios\n",
    "!pip install -r https://raw.githubusercontent.com/vcalderon2009/2021_06_Deep_Learning_tutorial/feature/refactoring/pkg_requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ca84f-98f8-40b5-97b3-d97ee811ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9311fab9-0ee6-4db8-947d-b2a0f3bfc807",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Primero bajamos la base de datos en la carpeta actual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554428a-b0fa-46fe-bab3-3b6075fce13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the Penn-Fudan dataset\n",
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
    "# extract it in the current folder\n",
    "!unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f57abe-5bd9-46a2-a2af-d832ab60f724",
   "metadata": {},
   "source": [
    "Ahora podemos ver la estructura de esta nueva carpeta:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e8c8c-4b88-4e45-ae6b-d7dc14a0caa2",
   "metadata": {},
   "source": [
    "```\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    FudanPed00003_mask.png\n",
    "    FudanPed00004_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    FudanPed00003.png\n",
    "    FudanPed00004.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c596dd9-7c76-4251-a6eb-8b888062be08",
   "metadata": {},
   "source": [
    "Cada una de estas imágenes tiene asociado las máscaras que enseñan en donde es que hay personas.\n",
    "\n",
    "Por ejemplo, podemos ver una de estas imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2c30c-1594-4c25-ad4d-ac98f1f9c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Número de imagen\n",
    "image_number = \"00003\"\n",
    "\n",
    "Image.open(f'PennFudanPed/PNGImages/FudanPed{image_number}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd92b7a-ab6e-48bd-a71f-37f2916b02c1",
   "metadata": {},
   "source": [
    "Y también podemos ver la **máscara** asociada a esta imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab75f2d7-71aa-4bc6-8298-0ef84083ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = Image.open(f'PennFudanPed/PedMasks/FudanPed{image_number}_mask.png')\n",
    "# cada instancia de máscara tiene un color diferente, de cero a N, donde\n",
    "# N es el número de instancias. Para facilitar la visualización,\n",
    "# agreguemos paleta de colores a la máscara.\n",
    "mask.putpalette([\n",
    "    0, 0, 0, # black background\n",
    "    255, 0, 0, # index 1 is red\n",
    "    255, 255, 0, # index 2 is yellow\n",
    "    255, 153, 0, # index 3 is orange\n",
    "])\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60a9d1-4e81-48b2-9a17-f28d9948363a",
   "metadata": {},
   "source": [
    "Esto significal que cada imagen tiene una máscara de segmentación asociada, en donde el color corresponde a las\n",
    "diferentes instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290537e7-b3dc-4607-b74e-c49be1ec4e3f",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "La siguiente parte es el **Dataset**, en donde se define como leer y ejecutar la data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4712c-9e38-49b9-8059-f12b2ccb5f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # Lee todas las imágenes y las ordena para ver que estén todas alineadas\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Leyendo las imágenes y máscaras\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # NOTE: tenga en cuenta que no hemos convertido la máscara a RGB,\n",
    "        # porque cada color corresponde a una instancia diferente\n",
    "        # siendo 0 el fondo\n",
    "        mask = Image.open(mask_path)\n",
    "\n",
    "        mask = np.array(mask)\n",
    "        # las instancias están codificadas como colores diferentes\n",
    "        obj_ids = np.unique(mask)\n",
    "        # la primera identificación es el fondo, así que elimínela\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # dividir la máscara codificada por colores en un conjunto\n",
    "        # de máscaras binarias\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # obtener las coordenadas del cuadro delimitador para cada máscara\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Solamente hay una clase para acá\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # supongamos que no todas las instancias son multitud\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2624ea-b40c-43db-86e0-976664356d39",
   "metadata": {},
   "source": [
    "Ahora que ya hemos definido el objecto del `Dataset`, podemos ver como se estructura los objetos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c611c-4c4a-4ee9-b161-d6f38bbda1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PennFudanDataset('PennFudanPed/')\n",
    "dataset[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e902ba0-0fad-4172-9732-9c7b211ff72b",
   "metadata": {},
   "source": [
    "Este objecto devuelve lo siguite:\n",
    "- La imágen en formato de `PIL.image`.\n",
    "- Un diccionario con el metadata de `boxes`, `labels`, y `masks`. Toda esta información se puede utilizar para entrenar un modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80c907-f22d-42c5-af56-dd97f5f416e6",
   "metadata": {},
   "source": [
    "### Modelaje\n",
    "\n",
    "En este tutorial, usaremos [Mask R-CNN](https://arxiv.org/abs/1703.06870), el cual está basado en la red neural [Faster R-CNN](https://arxiv.org/abs/1506.01497). Faster R-CNN\n",
    "es un modelo que predice tanto cuadros delimitadores como puntuaciones de clase para objetos potenciales en la imagen.\n",
    "\n",
    "![Faster R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image03.png)\n",
    "\n",
    "Mask R-CNN agrega una rama adicional a Faster R-CNN, que también predice máscaras de segmentación para cada instancia.\n",
    "\n",
    "![Mask R-CNN](https://raw.githubusercontent.com/pytorch/vision/temp-tutorial/tutorials/tv_image04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef93e5a-05ab-433c-b650-707131e56645",
   "metadata": {},
   "source": [
    "##### Entrenando el modelo\n",
    "\n",
    "Para este tutorial usaremos un modelo que ya ha sido entrenado previamente, y utilizaremos las pesas\n",
    "de este modelo para aplicarlo a nuestra base de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5802d2-6002-4db4-bb10-f95f2f80dfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos los módulos necesarios, como los modelos y demás\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # En este caso, estamos cargando ya un modelo entrenado de RESNET sobre la\n",
    "    # base de COCO.\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Obtenemos el número de características de entrada para el clasificador\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Reemplazamos la cabeza previamente entrenada por una nueva.\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Ahora podemos obtener el número de características de entrada para el clasificador de máscara\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd8acaa-2b03-4bf2-a9e2-271bac6bfd0b",
   "metadata": {},
   "source": [
    "Ahora necesitamos copiar unos archivos de `Pytorch` para usarlos acá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42eae3c-7b04-4c9e-ae86-d80f4ad2eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b v0.3.0 --single-branch https://github.com/pytorch/vision.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225aceb-e94f-48b9-9eed-53621108206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can copy files from the following location\n",
    "for file_ii in [\"utils.py\", \"transforms.py\", \"coco_eval.py\", \"engine.py\", \"coco_utils.py\"]:\n",
    "    shutil.copy(f\"./vision/references/detection/{file_ii}\", \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cb132-9179-4e66-9664-04edf26648cb",
   "metadata": {},
   "source": [
    "Y ahora podemos escribir las funciones para transformar las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26447b-d770-49a7-bb10-345728b320a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f339bdf5-c433-4526-90c7-6a5ff5e6db40",
   "metadata": {},
   "source": [
    "Ahora creamos el dataset total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797ca0e0-865b-4263-aba5-be38a8ce9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "torch.manual_seed(1)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a15c5e-cd8f-4dea-93f3-787a09ed44f3",
   "metadata": {},
   "source": [
    "Y empezamos el entrenamiento del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ae6ee-1651-4ef1-9c81-333b78837c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_instance_segmentation_model(num_classes)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef7dd9-a9c3-4b7f-8c63-1965b24c25e8",
   "metadata": {},
   "source": [
    "Iteramos sobre el diferente número de épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df18b4-efb3-4eb7-bcd2-87ccafcee745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be3068-51ca-4a1d-92d8-59e085bfe0f3",
   "metadata": {},
   "source": [
    "#### Evaluciación del modelo\n",
    "\n",
    "Ahora podemos ver que tan bien aprendió el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b04486-6fd1-41e6-b2e6-b8d30d375ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fbf09-1916-4138-8194-84e8d1c33238",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490c66a6-db01-431a-a293-b4dbfe252ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y esta es la máscara\n",
    "Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d958740-8894-4a31-abc5-9ce2a417300f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
